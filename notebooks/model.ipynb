{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong> XGBoost 모델 훈련 및 학습 </strong></h1>\n",
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "<h2> 1. Introduction </h2>\n",
    " <p style=\"font-size:16px\">개요 : 본 노트북은 프로젝트에서 사용될 파라미터 및 하이퍼파라미터 저장 과정을 기록한 리포트이다.\n",
    " </p>\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "<h2> 2. Library Import </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\") \n",
    "import params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "<h2> 3. XGBoost 모델 훈련 및 저장 함수 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset_path: str):\n",
    "    if not dataset_path or not os.path.exists(dataset_path):\n",
    "        print(\"[INFO] 데이터셋이 존재하지 않습니다.\")\n",
    "        return\n",
    "\n",
    "    print(\"[INFO] Started training...\")\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    X = df[['mean', 'std', 'high_freq_ratio']]\n",
    "    y = df['target_k']\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"[INFO] 학습 데이터: {len(X_train)}개, 검증 데이터: {len(X_val)}개\")\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    \n",
    "    print(\"[INFO] Training data...\")\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, preds)\n",
    "    print(f\"[INFO] Successfully completed. Test Set MSE: {mse:.6f}\")\n",
    "    \n",
    "    model_path = os.path.join(params.output_dir, \"xgb_model.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"[INFO] Saved the model : {model_path}\")\n",
    "\n",
    "    return model, X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px\">\n",
    "   위 함수는 XGBoost 모델의 하이퍼파라미터를 지정하고 학습시킨다.\n",
    "\n",
    "   이 모델의 특징(features)에는 각 패치별 평균 밝기, 분산, 고주파 비율 값들이 사용되고, 타겟(target)은 샤프닝강도 k값이다.\n",
    "\n",
    "   부스팅 모델의 하이퍼파라미터에 objective를 MSE로 설정하여 오차율을 최소화하는 것을목표로 했고, 결정 트리의 개수(n_estimators)는 1000개, 학습률(learning_rate)은 0.05로 설정하여 조금씩 학습하는 대신많은 트리의개수로 꾸준하고 정밀하게 정답에 접근하도록 의도했다.\n",
    "   \n",
    "   또한 트리의 최대 깊이(max_depth)는 5로 제한하고, 데이터와 특성(column)의 샘플링 비율을 각각 0.8로 설정하여 과적합을 방지하며 일반화 성능을 높였다.\n",
    "\n",
    "   특히 50회 동안 성능 개선이 없을 시 학습을 중단하는 조기 종료(early stopping_rounds) 조건을 추가했다. 데이터셋은 학습용과 검증용으로 8:2 비율로 분할하여 사용했으며, 학습 완료 후 검증 세트에 대한 평균 제곱 오차(MSE)를 통해 성능을 평가하고 최종 모델을 파일로 저장하도록 구현했다.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    X_train = X_train.drop(columns=['target_k'], errors='ignore')\n",
    "    X_val = X_val.drop(columns=['target_k'], errors='ignore')\n",
    "\n",
    "    y_pred_train = model.predict(X_train) # 학습한데이터 예측\n",
    "    y_pred_val = model.predict(X_val) # 새로운 데이터 예측\n",
    "\n",
    "    rmse_tr = np.sqrt(mean_squared_error(y_train, y_pred_train))    \n",
    "    rmse_va = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "    rmae_tr = np.sqrt(mean_absolute_error(y_train, y_pred_train))\n",
    "    rmae_va = np.sqrt(mean_absolute_error(y_val, y_pred_val))\n",
    "    \n",
    "    print(\"========== Evaluate ==========\")\n",
    "    print(\"RMSE of Train Set : \", rmse_tr)\n",
    "    print(\"RMSE of Test Set : \", rmse_va)\n",
    "    print()\n",
    "    print(\"RMAE of Train Set : \", rmae_tr)\n",
    "    print(\"RMAE of Test Set : \", rmae_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px\">\n",
    "위 함수는 학습된 모델의 성능을 검증하고, 과적합 여부를 판단하기 위해 지표를 계산하여 출력하는 함수이다.\n",
    "\n",
    "평균 제곱 오차의 제곱근, 평균 절대 오차의 제곱근을 사용했다.\n",
    "k값을 정확히 예측하는 것에 목표가 아닌 영역의 경향성을 예측하면 되기 때문에 결정계수(R2)나 다른 추가 지표는 사용하지 않았다.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h2> 4. 결론 </h2>\n",
    "<p style=\"font-size:16px\">\n",
    "- 과적합을 방지하기 위해 <code>learning_rate</code>와 <code>max_depth</code>, 샘플링 등의 하이퍼파라미터를 조정하였으며, 조기 종료 기법을 적용하여 성능을 향상시켰다.<br>\n",
    "- 평가지표로 RMSE, RMAE, R2로 과적합, 과격한 오차, 학습률을 비교했다.<br>\n",
    "    <br><br>\n",
    "\n",
    "이제 학습된 XGBoost 모델을 로드하여, 실제 저화질 이미지에 적응형 언샤프 마스킹(Adaptive Unsharp Masking)을 적용해보고 화질 개선 효과를 시각적으로 검증하는 시연 단계를 진행할 예정이다.\n",
    "\n",
    "    -> notebooks/inference.ipynb\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
